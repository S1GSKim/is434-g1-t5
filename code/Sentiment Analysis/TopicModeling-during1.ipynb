{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim (Topic Modeling Pacakge)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from spacy_download import load_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AndyVermaut' 'TrendzNewsbd' 'COVIDLive' 'AhmadNorMaulana'\n",
      " 'islandofcovid' 'suhail1mirza' 'yasu_yasuno_sa' 'eeeer71' 'nicktompan'\n",
      " 'BenHssinAhmed2' 'Mott_Mason' 'newsfilterio' 'GlobalPandemics'\n",
      " 'AquariusOnFire' 'VonPyotr' 'SharonShepshan' 'tazalot1981' 'latimes'\n",
      " 'Ravie777' 'schisano85' 'morning_news24h' 'CTVNews' 'unstoppable8672'\n",
      " 'boatratbown' 'Leedslad48' 'jlwoods0241' 'merapimanf' 'BobinSea'\n",
      " 'renztamayo' 'durindale' 'Hibatu3' 'alicuff' 'CGJr_carlos' 'yahata11'\n",
      " 'LouWasHe' 'andy68101744' 'mlauriat' 'DefaultNu' 'TraderMarcoCost'\n",
      " 'VivekSubbiah' 'meer10001' 'jonathanrockoff' 'JerrBoi' 'Alan_Nishihara'\n",
      " 'NuritBaytch' 'NZStuff' 'elvisjj' 'WatchOurCity' 'onelinernews123'\n",
      " 'CassieY4' 'SetTheoryVirus' 'ZyiteGadgets' 'bmangh' 'roto_rua'\n",
      " 'jayjames4' 'usatodaysun21' 'chuck85258' 'tjmakiboi' 'Su15100307'\n",
      " 'Melodys_Musings' 'jebender1' 'Reuters' 'caa1000' 'DennisKendel'\n",
      " 'TweeterSomebody' 'Nesgirl914' 'Nan10' 'thedextazlab' 'fox5dc' 'EGirald'\n",
      " 'MelanieMoore' 'easyOntario' 'JaxRaymond' 'ErieNewsNow' 'wbaltv11'\n",
      " 'RichinWriterss' 'patricksgeaney' 'Raphael_A_Bauer' 'bornblonde0077'\n",
      " 'US_FDA' 'SocialistVoice' 'PAconsultants1' 'mcbc' 'apokerplayer'\n",
      " 'sapp_erlot' 'alexselby1770' 'Okie_Resister' 'DrAnthony' 'HammerToe'\n",
      " 'corndnc1' 'KodyKinsley' 'DrEricDing' '1NewsNZ' 'openletterbot'\n",
      " 'helenbinchy' 'AstorAaron' 'Teelin' 'leemorson3NUFC' 'NedPagliarulo'\n",
      " 'RobButl69654384' 'RavenHawk4' 'TheHinduScience' 'thehill'\n",
      " 'JulianneMcShane' 'drpestuk' 'sauceocho' 'ReginaPhelps' 'AlyeskaDawn'\n",
      " 'MarianneSkolek' 'dubvNOW' 'thinkpolca' 'TALK1370' 'PulpNews' '509ShawnG'\n",
      " 'chuckghunter' 'ashishkjha' 'MHGtweet' 'BuckeyeFineArts' 'HindustanTimes'\n",
      " 'MikeTaddow' 'jools6691' 'jt120651' 'LarcombePeter' '_avasharpe'\n",
      " 'civility1799' 'TimesofNewsHUB' 'bba7422847' 'unfard' 'bottlecapkingnj'\n",
      " 'feedpushr' 'CHOMcgoo' 'maryaalexand1' 'aldotjahjadi8' 'Djfury4412'\n",
      " 'StephenGutowski' 'Care2much18' 'tlscadden1' 'PIX11News' 'thomaskaine5'\n",
      " 'HealthNews_b' 'itrixy' 'meister_mitch' 'koconews' 'FriedrichHayek'\n",
      " 'Chris_1791' 'debbiemac603' '82669F' 'darlenekrause35' 'globalissuesweb'\n",
      " 'TinkerChip' 'Jerusalem_Post' 'EdCara4' 'Trendypedia2' 'AndrewMakeTweet'\n",
      " 'DavidMPeirce1' 'wacotrib' 'LNevilllle' 'BulletinCurrent' 'mailmenow'\n",
      " 'CBSPhiladelphia' 'q0mega' 'skepticalraptor' 'zyiteblog'\n",
      " 'Laughatsaltyge1' 'AliezeMac' 'Reuters_Health' 'VIXC_News' 'namenzie'\n",
      " 'Beepr5' 'BNOFeed' 'ralphmorrison22' 'guardiannews' 'ArmstrongDrew'\n",
      " 'CNBC' 'JudexXavier' '_DrFrusci' 'tobbystweet' 'seashawol' 'LatinoLdnOnt'\n",
      " 'zryanverse' 'qctimes' 'Mikejcl52' 'Justin_EAndrews' 'inkylu_lu'\n",
      " 'Johnjenkins309' 'sondra_watkins' 'HesterMcQueen' 'ecobearwitness'\n",
      " 'allenfolsom3' 'PicoDeGallo14' 'norionakatsuji' 'BARDA' 'joedunford200'\n",
      " 'canisgallicus' 'WXII' 'BabeTruth2' 'hcextrak' 'scj']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime          147389\n",
       "tweet_id          147389\n",
       "text              147389\n",
       "username          147389\n",
       "url               147389\n",
       "user_location     117081\n",
       "like_count        147389\n",
       "retweet_count     147389\n",
       "follower_count    147389\n",
       "reply_count       147389\n",
       "verified          147389\n",
       "hashtags           27728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will be using the 20-Newsgroups dataset for this exercise.\n",
    "This version of the dataset contains about 11k newsgroups posts from \n",
    "20 different topics. This is available as newsgroups.json.\n",
    "'''\n",
    "# Import Dataset\n",
    "df = pd.read_csv('data/during1_covidTweets_v2.csv')\n",
    "print(df.username.unique())\n",
    "df.head()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US Authorizes Johnson &amp; Johnson Covid Vaccine For Emergency Use ']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As you can see there are many emails, newline and extra spaces that is quite distracting.\n",
    "Let’s get rid of them using regular expressions.\n",
    "'''\n",
    "# Convert to list\n",
    "data = df.text.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "#remove URL\n",
    "data = [re.sub('http://\\S+|https://\\S+', '', sent) for sent in data]\n",
    "data = [re.sub('http[s]?://\\S+', '', sent) for sent in data]\n",
    "data = [re.sub(r\"http\\S+\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['us', 'authorizes', 'johnson', 'amp', 'johnson', 'covid', 'vaccine', 'for', 'emergency', 'use']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us', 'authorizes', 'johnson', 'amp', 'johnson', 'covid', 'vaccine', 'for', 'emergency_use']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will download the model if it isn't installed yet \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(id2word.token2id)\n",
    "\n",
    "pd.Series([t for tokens in data_words_bigrams for t in tokens if \"_\" in t]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model perplexity and topic coherence provide a convenient measure to judge \n",
    "how good a given topic model is. In my experience, topic coherence score, \n",
    "in particular, has been more helpful.\n",
    "'''\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that the LDA model is built, the next step is to examine the produced \n",
    "topics and the associated keywords. There is no better tool than pyLDAvis package’s \n",
    "interactive chart and is designed to work well with jupyter notebooks.\n",
    "'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "\n",
    "# np.random.seed(7)\n",
    "pn.extension('tabulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layout using Template\n",
    "template = pn.template.FastListTemplate(\n",
    "    title='Covid-19 Social Analytics', \n",
    "    sidebar=[pn.pane.Markdown(\"# Misinformation and Sentiment Analysis\"), \n",
    "             pn.pane.Markdown(\"#### Carbon dioxide emissions are the primary driver of global climate change. It’s widely recognised that to avoid the worst impacts of climate change, the world needs to urgently reduce emissions. But, how this responsibility is shared between regions, countries, and individuals has been an endless point of contention in international discussions.\"), \n",
    "             pn.pane.PNG('climate_day.png', sizing_mode='scale_both'),\n",
    "             pn.pane.Markdown(\"## Settings\")],\n",
    "    main=[pn.Row(pn.Column(plt.panel(width=700), margin=(0,25)), \n",
    "                 plt2.panel(width=500)), \n",
    "         pn.Row(pn.Column(plt.panel(width=700), margin=(0,25)), \n",
    "                 plt2.panel(width=500))],\n",
    "    accent_base_color=\"#88d8b0\",\n",
    "    header_background=\"#88d8b0\",\n",
    ")\n",
    "# template.show()\n",
    "template.servable();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

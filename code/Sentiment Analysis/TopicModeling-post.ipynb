{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim (Topic Modeling Pacakge)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "from spacy_download import load_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RochelleAdame7' 'brian_tietz' 'LarryBoorstein' 'VigilantFox'\n",
      " 'Mrmangajojo' 'HawkInATX' 'vindicator1337' 'KoonsKarl' 'JohnnyRode'\n",
      " 'Oneguy83440339' 'Excuseeeme' 'StuckInSanDiego' 'JudithArnett1'\n",
      " 'Tigerlily1011' 'Deevoee' 'Katetheskates' 'LankeyPants250'\n",
      " 'FellwockPerry' 'most_articulate' 'Smpwns' 'USBornNRaised' 'AlisynGayle'\n",
      " 'RobinBr83683748' 'urbiasdqsu' 'adelaidians' 'mrmikebones' '_LoveLiberty'\n",
      " 'LonelyFreedom' 'ClimateDepot' 'CharmsYahoo' 'STRAT777' 'Salty_shifter'\n",
      " 'JamieO123' 'Faketriots' 'CristinoCastro9' '76Americanwomen'\n",
      " 'io_aircraft_inc' 'MrMarchi' 'KingGrunt' 'MrSkanner' 'PatrioticPatty2'\n",
      " 'SherryHighHorse' 'DailyReformerUS' 'TruckCastreauJr' 'valentino_nora'\n",
      " 'OrmeStephan' 'JohnT43838185' 'Kellyeod' 'DennisOgden007' 'Alec121212'\n",
      " 'JackHay01988146' 'small4lyfe' 'Allison86435394' 'nomandates23'\n",
      " 'andriedeja' 'ImmunifyLife' 'tomcoates' 'caz_sampson' 'Crof'\n",
      " 'Amfirstnewswire' 'WoodyWhitcomb' 'GeorgeG55508748' 'livejelly'\n",
      " 'BuzzerlyApp' 'kyle8824' 'stephphilip8' 'infamous_hippo' 'rpoquinn71'\n",
      " 'virtusignals' 'james_mclamb' 'fleewickedneas' 'LeviDennis18' 'tetuben'\n",
      " 'Fallen_SeeD' 'Chester_16_11' 'StillMask' 'HardBoiledEddie'\n",
      " 'brigitt07009853' 'RooDay10' 'tamanegiR41120' 'meganh3121' 'uneektweet'\n",
      " 'dumbassgenius' 'MurielBlaivePhD' 'playagotslayed' 'frobos'\n",
      " 'BelleTower88' 'ANGUS83016501' 'burn_212' 'barrykewg' 'Laura743717542'\n",
      " 'a_clunk' 'BoucherH20_' 'RKwan54440056' 'dwright100' 'dense_evi'\n",
      " 'DebbieSVA' 'Rome_Colt45' 'KelfontStudio' 'LukeWar73493682' 'DSMWcom'\n",
      " 'BeaversCanKill' 'michaelquestio1' 'reneroblesce' 'MandyRox77'\n",
      " 'BluebonnetDepl1' 'TheNewBailey' 'cabville' 'krishna444_test' 'shllymoss'\n",
      " 'Joe_Jinping' 'alitlstrawberry' 'AmericaProud17' 'bioethicsdotcom'\n",
      " 'NowItsaThought' 'brockingtonr74' 'IngeniousGal' 'BrainSightsApp'\n",
      " 'SteveCa31353955' 'GOPReaper1' 'RetiredMilGuy' 'PopPopPopKultur'\n",
      " 'vancemurphy' 'LeonardoAlban13' 'JamesDitto12' 'DrewInSeAsia' 'OccleGrag'\n",
      " 'NjbBari3' 'AmyCookOC' 'NeedleNoseNed99' 'MickyMayon' 'ValeLeegirl'\n",
      " 'PatriotTownUSA' 'sandya418' 'LeeRobin19' 'PatriciaTursi'\n",
      " 'ColourBuddy369' 'AlisonJury' 'seattlemediaguy' 'inaLaanDheere'\n",
      " 'RuthB996' 'Th0rKnight' 'PolitiBunny' 'MaisaCorp' 'MarkAll26740409'\n",
      " 'notonmywatchfyi' 'AndyGreensky' 'yeahnaa333' 'SavvaJ26' 'YapSoonchia1'\n",
      " 'AceSNZ' 'LTrueWest' 'touchdownjets51' 'e_galv' 'DuncanMcFarlan'\n",
      " 'TRAVISLWAGNER11' 'peterk0578' 'afshineemrani' 'Dave66rivi1'\n",
      " 'MirageNewsCom' 'NewsNation' 'Newsfop1' 'TropicalVertic1' 'ThinkyTexan'\n",
      " 'IiiDamack' '_jake_tweets' 'smartwookiee' 'ElonUnblockMe' '_Freelander'\n",
      " 'Tracey_5591' 'BradLangley1957' 'AshLaughs79' 'cjowalter' 'real_GGoswami'\n",
      " 'GiraffeDee' '_ppmv' 'donaldmcc56' 'slicky1955' 'bestqueenliz'\n",
      " 'WilsonC57102397' 'kennydojo' 'Chuck67902819' 'concettaeedy' 'SCOURING15'\n",
      " 'Adams6841' 'FrontierNZmedia' 'SeMor93043787' 'VanCityKal' 'RS_COVID19'\n",
      " 'RockyRhoades10' 'savagecookie' 'VictoriaEAlizo' 'NewsSairan' 'Joyenz1'\n",
      " 'thatsnotfunny10' 'mareefeb' 'RayDoesData' 'BansheeAutoPart']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime          40886\n",
       "tweet_id          40886\n",
       "text              40886\n",
       "username          40886\n",
       "url               40886\n",
       "user_location     27639\n",
       "like_count        40886\n",
       "retweet_count     40886\n",
       "follower_count    40886\n",
       "reply_count       40886\n",
       "verified          40886\n",
       "hashtags          14700\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We will be using the 20-Newsgroups dataset for this exercise.\n",
    "This version of the dataset contains about 11k newsgroups posts from \n",
    "20 different topics. This is available as newsgroups.json.\n",
    "'''\n",
    "# Import Dataset\n",
    "df = pd.read_csv('data/post_covidTweets_v2.csv')\n",
    "print(df.username.unique())\n",
    "df.head()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Did I say it was evil? Did I say dont get it? NEWSFLASH....I DIDNT. And I '\n",
      " 'get the flu shot every year and my mammogram in the same week until '\n",
      " 'recently. I have never been told I had to postpone my mammogram for any '\n",
      " 'other vaccine. NOT ONE.']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As you can see there are many emails, newline and extra spaces that is quite distracting.\n",
    "Letâ€™s get rid of them using regular expressions.\n",
    "'''\n",
    "# Convert to list\n",
    "data = df.text.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "#remove URL\n",
    "data = [re.sub('http://\\S+|https://\\S+', '', sent) for sent in data]\n",
    "data = [re.sub('http[s]?://\\S+', '', sent) for sent in data]\n",
    "data = [re.sub(r\"http\\S+\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['did', 'say', 'it', 'was', 'evil', 'did', 'say', 'dont', 'get', 'it', 'newsflash', 'didnt', 'and', 'get', 'the', 'flu', 'shot', 'every', 'year', 'and', 'my', 'mammogram', 'in', 'the', 'same', 'week', 'until', 'recently', 'have', 'never', 'been', 'told', 'had', 'to', 'postpone', 'my', 'mammogram', 'for', 'any', 'other', 'vaccine', 'not', 'one']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did', 'say', 'it', 'was', 'evil', 'did', 'say', 'dont', 'get', 'it', 'newsflash', 'didnt', 'and', 'get', 'the', 'flu', 'shot', 'every', 'year', 'and', 'my', 'mammogram', 'in', 'the', 'same', 'week', 'until', 'recently', 'have', 'never', 'been', 'told', 'had', 'to', 'postpone', 'my', 'mammogram', 'for', 'any', 'other', 'vaccine', 'not', 'one']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will download the model if it isn't installed yet \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(id2word.token2id)\n",
    "\n",
    "pd.Series([t for tokens in data_words_bigrams for t in tokens if \"_\" in t]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model perplexity and topic coherence provide a convenient measure to judge \n",
    "how good a given topic model is. In my experience, topic coherence score, \n",
    "in particular, has been more helpful.\n",
    "'''\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that the LDA model is built, the next step is to examine the produced \n",
    "topics and the associated keywords. There is no better tool than pyLDAvis packageâ€™s \n",
    "interactive chart and is designed to work well with jupyter notebooks.\n",
    "'''\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
